<!DOCTYPE html>
<html>


<head>
  <meta charset="utf-8">
  <meta name="description" content="Are We on the Right Way for Evaluating Large Vision-Language Model?">
  <meta name="keywords" content="MMStar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMStar</title>

  <link rel="icon" href="images/logo.png">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script type="text/javascript" src="./static/js/sort-table.js" defer></script>
  <script src="./static/js/fontawesome.all.min.js" defer></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>
  <script src="./static/js/leaderboard_testmini.js"></script>  
</head>


<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
      <div class="navbar-dropdown">
        <a class="navbar-item" href="https://sharegpt4v.github.io/">
          <b><img src="images/sharegpt4v.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>ShareGPT4V</b>
        </a>
        <a class="navbar-item" href="https://sharegpt4video.github.io/">
          <b><img src="images/Share4video.jpg" style="width:2.0em;vertical-align: middle" alt="Logo"/>ShareGPT4Video</b>
        </a>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h2 class="title is-2 publication-title"><img id="logo" width="5%" src="images/logo.png"> MMStar</h2> -->
          <h1 class="title is-1 publication-title">Are We on the Right Way for Evaluating <br /> Large Vision-Language Models?</h1>
          <font size="5"><span style="color: red; font-weight: bold;">NeurIPS 2024</span></font>
          <div class="is-size-5 publication-authors">
            <br>
            <span class="author-block">
              <a href="https://lin-chen.site/" style="font-weight:normal;">Lin Chen<b><sup>* &sect;1,3</sup></b></a>,
            </span>
            <span class="author-block">
              <a href="https://li-jinsong.github.io/" style="font-weight:normal;">Jinsong Li<b><sup>* &sect;2,3</sup></b></a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=FscToE0AAAAJ&hl=en" style="font-weight:normal;">Xiaoyi Dong<sup>2,3</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://panzhang0212.github.io/" style="font-weight:normal;">Pan Zhang<sup>3</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://yuhangzang.github.io/" style="font-weight:normal;">Yuhang Zang<sup>3</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://lovesnowbest.site/" style="font-weight:normal;">Zehui Chen<sup>1</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://kennymckormick.github.io/" style="font-weight:normal;">Haodong Duan<sup>3</sup></a>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://myownskyw7.github.io/" style="font-weight:normal;">Jiaqi Wang<b><sup>&dagger;3</sup></b></a>,
            </span>
            <span class="author-block">
              <a href="https://mmlab.siat.ac.cn/yuqiao" style="font-weight:normal;">Yu Qiao<sup>3</sup></a>,
            </span>
            <span class="author-block">
              <a href="http://dahua.site/" style="font-weight:normal;">Dahua Lin<sup>2,3</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en" style="font-weight:normal;">Feng Zhao<b><sup>&dagger;1</sup></b></a>
            </span>
          </div>
          
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#003FA8; font-weight:normal">&#x25B6 </b> <sup>1</sup> University of Science and Technology of China</span>
            <span class="author-block"><b style="color:#D79634; font-weight:normal">&#x25B6 </b> <sup>2 </sup> The Chinese University of Hong Kong</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#149BE9; font-weight:normal">&#x25B6 </b> <sup>3 </sup> Shanghai Artificial Intelligence Laboratory</span>
          </div>
          
          <div class="is-size-6 publication-authors">
            <br>
            <span class="author-block"><b>*</b> Equal contribution.</span>
            <span class="author-block"><b>&dagger;</b> Corresponding authors.</span>
          </div>
          
          <div class="is-size-6 publication-authors">
            <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Shanghai Artificial Intelligence Laboratory.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.20330" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/MMStar-Benchmark/MMStar" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Lin-Chen/MMStar" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">🤗</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#Leaderboard Title"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🏆</p>
                  </span>
                  <span>MMStar Leaderboard</span>
                </a>
              </span>
            </div>
            <font size="4">
              <br>🚀 Two Key Issues that Lead to <b>Misjudgment of LVLM Capability</b>
              <br>🚀 An Elite Vision-indispensable Multi-modal Benchmark, <img src="images/logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><b>MMStar</b>.
              <br>🚀 Two Metrics: <b>Multi-modal Gain (MG)</b> and <b>Multi-modal Leakage (ML)</b>
            </font>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <font size="5">
      <div class="has-text-centered">
        <p>🔥<b>What's New</b></p>
      </div>
    </font>
    <font size="4">
      <table width="75%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
        <tr>
          <td>
            <div style="height: 150px; overflow: auto;">
              <ul> 
                <li> <span style="color: red; font-weight: bold;">[2024.09.26] 🎉🎉🎉 MMStar is accepted by NeurIPS 2024!</span>
                <li> [2024.04.16] 🚀 MMStar has been supported in the <a href="https://github.com/open-compass/VLMEvalKit">VLMEvalKit</a> repository and <a href="https://rank.opencompass.org.cn/leaderboard-multimodal">OpenCompass</a> leaderboard!</b>
                <li> [2024.04.02] 🚀 Huggingface Dataset and evaluation code are available!</b>
                <li> [2024.04.01] 🚀 We released the ArXiv paper.</b>
              </ul>
            </div>
          </td>
        </tr>
      </tbody>
    </table>
  </font>
  </div>
</section>


<section class="section" id="Abstract">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities.
            However, we dig into current evaluation works and identify two primary issues: 
            <br /><b>1) Visual content is unnecessary for many samples.</b> The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over 20% on average.
            <br /><b>2) Unintentional data leakage exists in LLM and LVLM training.</b> LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%.
            <br /><b>Both problems lead to misjudgments of actual multi-modal performance gains and potentially misguide the study of LVLM.</b> 
            <br />
            <br />To this end, we present <b>MMStar</b>, an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. MMStar is designed to benchmark 6 core capabilities and 18 detailed axes, aiming to evaluate the multi-modal capacities of LVLMs with a carefully balanced and purified selection of samples.
            The samples are first roughly selected from current benchmarks with an automated pipeline, strict human review is then involved to ensure each selected sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities for the solution.
            In addition to the traditional accuracy metric, we also develop two metrics to measure data leakage and actual performance gain in multi-modal training. 
            We evaluate 16 leading LVLMs on our MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the newly proposed metrics to investigate their data leakage and actual multi-modal gain. 
            We hope these efforts can serve as a valuable addition to the research community in evaluating LVLMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small" id="Issues Title">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <img src="images/llm.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">The Overlooked Issues for Evaluating LVLMs</span>
  </h1>
  </div>
</section>
<section class="section" id="Issues">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p><b>
            We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual
            dependency or have unintentionally leaked into the training data of LLMs and LVLMs. 
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="100%" src="images/4_case_in_1.png">     
              </div>
            </centering>
            <br />(a) Some samples can be answered by LLMs using only text-based world knowledge; 
            <br />(b) For some instances, the question itself contains the answer, making images superfluous; 
            <br />(c) Some samples are leaked into LLMs' training corpora can be "recalled" with the textual questions and answers directly; 
            <br />(d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.
          </b></p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <h3 class="title is-4">Cases of Lacking Visual Dependency (a & b)</h3>
                <img src="images/cases_llm_1.png" alt="llm_1" width="80%"/>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <h3 class="title is-4">Cases of Data Leakage in LLMs (c)</h3>
                <img src="images/cases_llm_2.png" alt="llm_2" width="80%"/>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <h3 class="title is-4">Cases of Data Leakage during LVLMs' Multi-modal Training Processes (d)</h3>
                <img src="images/cases_pairs_3.png" alt="pairs_1" width="80%"/>
              </div>
            </div>
          </div>
        </div> 
      </div>
    </div>
  </div>
</section>
<section class="section" id="Table">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard"><span class="mathvista" style="vertical-align: middle">Evaluation of various LVLMs on 6 popular multi-modal benchmarks</span></h2>
        <div class="content">
          For the "strategy" column,
          <b>"LLM"</b> refers to evaluating using the corresponding LLM base of the LVLM, 
          while <b>"LVLM-text"</b> denotes evaluating LVLMs without accessing images. 
          <br>We employ the 0-shot inference strategy for LLMs to align the evaluation protocols of LVLMs. 
          <br>The <u><b>highest</b></u> results of the LVLM-text setting across the models are highlighted in <u><b>bold and underlined</b></u>.
          <br><b>For the entire LVLMs' results, please refer to the <u><a href="https://arxiv.org/pdf/2403.20330.pdf">appendix</a></u></b>.
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="60%" src="images/LVLM_on_6bench.png">     
            </div>
          </centering>
          
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small" id="MMStar Title">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <img src="images/logo.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">MMStar Benchmark</span>
  </h1>
  </div>
</section>
<section class="section" id="MMStar">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p><b>
            After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="90%" src="images/data_source.png">     
              </div>
            </centering>
            <br>
            In MMStar, we display 6 core
            capabilities in the inner ring, with 18 detailed axes presented in the outer ring. The middle ring showcases the
            number of samples for each detailed dimension. Each core capability contains a meticulously balanced 250
            samples. We further ensure a relatively even distribution across the 18 detailed axes.
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="60%" src="images/mmstar.png">     
              </div>
            </centering>
          </b></p>
        </div> 
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small" id="Leaderboard Title">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <img src="images/logo.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">MMStar Leaderboard</span>
  </h1>
  </div>
</section>
<section class="section" id="Leaderboard">
  <div class="container"> 
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <div class="content">
          CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science & technology), MA (mathematics).
          <br>The <u><b>best</b></u> results are highlighted in <u><b>bold and underlined</b></u>.
            The <b class="best-score-text">worst</b> results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in <b class="best-score-text">bold and red</b>.
          <table class="js-sort-table" id="results">
            <thead>
              <tr>
                <th rowspan="1" style="vertical-align: middle;"><strong>#</strong></th>
                <th rowspan="1" style="vertical-align: middle;"><strong>Model</strong></th>
                <th rowspan="1" style="vertical-align: middle;"><strong>LLM</strong></th>
                <th rowspan="1" style="vertical-align: middle;"><strong>Param.</strong></th>
                <th rowspan="1" style="vertical-align: middle;"><strong>CP</strong></th>
                <th colspan="1" style="vertical-align: middle;"><strong>FP</strong></th>
                <th colspan="1" style="vertical-align: middle;"><strong>IR</strong></th> <!-- Merging two columns for 'OOO' -->
                <th colspan="1" style="vertical-align: middle;"><strong>LR</strong></th> <!-- Merging two columns for 'PPP' -->
                <th colspan="1" style="vertical-align: middle;"><strong>ST</strong></th>
                <th colspan="1" style="vertical-align: middle;"><strong>MA</strong></th>
                <th colspan="1" style="vertical-align: middle;"><strong>Avg.✅</strong></th>
                <th colspan="1" style="vertical-align: middle;"><strong>MG⬆️</strong></th>
                <th colspan="1" style="vertical-align: middle;"><strong>ML⬇️</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>1</td>
                <td><b><a href="https://openai.com/" style="color: black;">GPT4V (high) 🥇</a></b></td>
                <td>GPT4-Turbo</td>
                <td> - </td>
                <td><u><b>76.6</b></u></td>
                <td>51.4</td>
                <td><u><b>66.6</b></u></td>
                <td>55.8</td>
                <td><u><b>42.6</b></u></td>
                <td>49.8</td>
                <td><u><b>57.1</b></u></td>
                <td><u><b>43.6</b></u></td>
                <td>1.3</td>                              
              </tr>
              <tr>
                <td>2</td>
                <td><b><a href="https://github.com/InternLM/InternLM-XComposer" style="color: black;">InternLM-XComposer2 🥈</a></b></td>
                <td>InternLM2-7B</td>
                <td>7B</td>
                <td>70.8</td>
                <td>48.8</td>
                <td>65.2</td>
                <td><u><b>56.4</b></u></td>
                <td>42.0</td>
                <td>49.2</td>
                <td>55.4</td>
                <td>28.1</td>
                <td>7.5</td>                             
              </tr>
              <tr>
                <td>3</td>
                <td><b><a href="https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf" style="color: black;">LLaVA-Next 🥉</a></b></td>
                <td>NH2-Yi-34B</td>
                <td>34B</td>
                <td>66.4</td>
                <td><u><b>52.0</b></u></td>
                <td>62.4</td>
                <td>46.0</td>
                <td>32.4</td>
                <td><u><b>53.6</b></u></td>
                <td>52.1</td>
                <td>29.4</td>
                <td>2.4</td>                         
              </tr>
              <tr>
                <td>4</td>
                <td><a href="https://openai.com/" style="color: black;"><b class="">GPT4V (low)</b></a></td>
                <td>GPT4-Turbo</td>
                <td> - </td>
                <td>62.0</td>
                <td>32.8</td>
                <td>55.2</td>
                <td>48.0</td>
                <td>33.6</td>
                <td>44.8</td>
                <td>46.1</td>
                <td>32.6</td>
                <td>1.3</td>                              
              </tr>
              <tr>
                <td>5</td>
                <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-Chinese-V1-2" style="color: black;"><b>InternVL-Chat-v1.2</b></a></td>
                <td>NH2-Yi-34B</td>
                <td>40B</td>
                <td>67.6</td>
                <td>43.2</td>
                <td>61.2</td>
                <td>47.2</td>
                <td>24.0</td>
                <td>19.2</td>
                <td>43.7</td>
                <td>32.6</td>
                <td><u><b>0.0</b></u></td>                               
              </tr>
              <tr>
                <td>6</td>
                <td><a href="https://gemini.google.com/" style="color: black;"><b>GeminiPro-Vision</b></a></td>
                <td>GeminiPro</td>
                <td> - </td>
                <td>51.6</td>
                <td>28.8</td>
                <td>50.8</td>
                <td>46.0</td>
                <td>28.4</td>
                <td>50.0</td>
                <td>42.6</td>
                <td>27.4</td>
                <td><u><b>0.0</b></u></td>                                  
              </tr>
              <tr>
                <td>7</td>
                <td><a href="https://huggingface.co/openbmb/MiniCPM-V-2" style="color: black;"><b>MiniCPM-V-2</b></a></td>
                <td>MiniCPM-2B-sft</td>
                <td>2B</td>
                <td>60.4</td>
                <td>31.6</td>
                <td>45.2</td>
                <td>45.2</td>
                <td>29.2</td>
                <td>32.4</td>
                <td>40.7</td>
                <td>18.7</td>
                <td>3.5</td>                                  
              </tr>
              <tr>
                <td>8</td>
                <td><a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory/blob/main/SPHINX" style="color: black;"><b>Sphinx-X-MoE</b></a></td>
                <td>Mixtral-8x7b</td>
                <td>57B</td>
                <td>58.4</td>
                <td>40.8</b></td>
                <td>47.6</td>
                <td>35.2</td>
                <td>19.2</td>
                <td>32.0</td>
                <td>38.9</td>
                <td>14.8</td>
                <td>1.0</td>                                
              </tr>
              <tr>
                <td>9</td>
                <td><a href="https://huggingface.co/echo840/Monkey-Chat" style="color: black;"><b>Monkey-Chat</b></a></td>
                <td>Qwen-7B</td>
                <td>10B</td>
                <td>57.6</td>
                <td>36.4</td>
                <td>51.6</td>
                <td>33.2</td>
                <td>26.4</td>
                <td>24.4</td>
                <td>38.3</td>
                <td>13.5</td>
                <td><b class="best-score-text">17.6</b></td>                        
              </tr>
              <tr>
                <td>10</td>
                <td><a href="https://huggingface.co/01-ai/Yi-VL-6B" style="color: black;"><b>Yi-VL</b></a></td>
                <td>Yi-6B</td>
                <td>6B</td>
                <td>58.0</td>
                <td>33.6</td>
                <td>46.4</td>
                <td>34.8</td>
                <td>20.4</td>
                <td>34.0</td>
                <td>37.9</td>
                <td>15.6</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>11</td>
                <td><a href="https://huggingface.co/Qwen/Qwen-VL-Chat" style="color: black;"><b>Qwen-VL-Chat</b></a></td>
                <td>Qwen-7B</td>
                <td>8B</td>
                <td>59.6</td>
                <td>32.0</td>
                <td>50.8</td>
                <td>29.2</td>
                <td>22.0</td>
                <td>31.6</td>
                <td>37.5</td>
                <td>23.9</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>12</td>
                <td><a href="https://huggingface.co/deepseek-ai/deepseek-vl-7b-chat" style="color: black;"><b>Deepseek-VL</b></a></td>
                <td>Deepseek-7B</td>
                <td>8B</td>
                <td>64.0</td>
                <td>30.8</td>
                <td>49.2</td>
                <td>36.4</td>
                <td>21.6</td>
                <td>20.4</td>
                <td>37.1</td>
                <td>15.7</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>13</td>
                <td><a href="https://huggingface.co/THUDM/cogvlm-chat-hf" style="color: black;"><b>CogVLM-Chat</b></a></td>
                <td>Vicuna-v1.5-7B</td>
                <td>17B</td>
                <td>66.8</td>
                <td>36.8</td>
                <td>49.2</td>
                <td>31.2</td>
                <td>23.6</td>
                <td>11.6</td>
                <td>36.5</td>
                <td>14.9</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>14</td>
                <td><a href="https://huggingface.co/01-ai/Yi-VL-34B" style="color: black;"><b>Yi-VL</b></a></td>
                <td>Yi-34B</td>
                <td>34B</td>
                <td>53.2</td>
                <td>31.2</td>
                <td>52.0</td>
                <td>32.4</td>
                <td>12.4</td>
                <td>35.2</td>
                <td>36.1</td>
                <td>18.8</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>15</td>
                <td><a href="https://huggingface.co/bczhou/tiny-llava-v1-hf" style="color: black;"><b>TinyLLaVA</b></a></td>
                <td>Phi2-2.7B</td>
                <td>3B</td>
                <td>60.4</td>
                <td>31.6</td>
                <td>50.8</td>
                <td>30.4</td>
                <td>18.0</td>
                <td>24.8</td>
                <td>36.0</td>
                <td>16.4</td>
                <td>7.6</td>                             
              </tr>
              <tr>
                <td>16</td>
                <td><a href="https://huggingface.co/Lin-Chen/ShareGPT4V-7B" style="color: black;"><b>ShareGPT4V</b></a></td>
                <td>Vicuna-v1.5-7B</td>
                <td>7B</td>
                <td>58.8</td>
                <td>28.0</td>
                <td>45.6</td>
                <td>24.4</td>
                <td>17.2</td>
                <td>24.0</td>
                <td>33.0</td>
                <td>11.9</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>17</td>
                <td><a href="https://huggingface.co/liuhaotian/llava-v1.5-13b" style="color: black;"><b>LLaVA-1.5</b></a></td>
                <td>Vicuna-v1.5-13B</td>
                <td>13B</td>
                <td>58.8</td>
                <td>28.0</td>
                <td>41.6</td>
                <td>24.4</td>
                <td>18.4</td>
                <td>25.6</td>
                <td>32.8</td>
                <td>13.9</td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>18</td>
                <td><a href="https://huggingface.co/liuhaotian/llava-v1.5-7b" style="color: black;"><b>LLaVA-1.5</b></a></td>
                <td>Vicuna-v1.5-7B</td>
                <td>7B</td>
                <td>58.8</td>
                <td>24.0</td>
                <td>38.8</td>
                <td>24.0</td>
                <td>13.6</td>
                <td>22.8</td>
                <td>30.3</td>
                <td><b class="best-score-text">10.7</b></td>
                <td><u><b>0.0</b></u></td>                             
              </tr>
              <tr>
                <td>19</td>
                <td><u><b>Random Choice</b></u></td>
                <td> - </td>
                <td> - </td>
                <td>23.7</td>
                <td>24.5</td>
                <td>25.3</td>
                <td>24.3</td>
                <td>24.8</td>
                <td>25.1</td>
                <td>24.6</td>
                <td> - </td>
                <td> - </td>                             
              </tr>
          </tbody>                                                   
        </table>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">📃 BibTeX</h2>
    <pre><code>
      @article{chen2024we,
        title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
        author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
        journal={arXiv preprint arXiv:2403.20330},
        year={2024}
      }
    </code></pre>
    <br>
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=5fkWaYaXIFBjo3ZXLy9Im1_uvPCz7ekv5UGtkp5SEiE&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
  </div>
</section>


<footer class="footer">
  <div class="content has-text-centered">
  </div>
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is adapted from <a href="https://mathvista.github.io/">MathVista</a> and <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>


</html>
